{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4684c2d5",
   "metadata": {},
   "source": [
    "# Training a Handwritten Kanji Recognition Model with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7914453b",
   "metadata": {},
   "source": [
    "## 1. Dataset Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9123eec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import our custom modules\n",
    "from load import KanjiLMDBDataset, get_transforms, create_dataloader, get_dataloaders, get_num_classes\n",
    "from clean import crop_and_pad, convert_to_bw\n",
    "from jis_unicode_map import jis_to_unicode\n",
    "\n",
    "# Set paths\n",
    "data_dir = '/app/output/prep'  # This is where our LMDB datasets are stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a0dacb",
   "metadata": {},
   "source": [
    "## 2. Transformations and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6143ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for training and validation\n",
    "train_transform = get_transforms(split='train', augment=True)\n",
    "val_transform = get_transforms(split='val', augment=False)\n",
    "\n",
    "# Create dataloaders using our utility function\n",
    "dataloaders = get_dataloaders(\n",
    "    data_dir=data_dir,\n",
    "    batch_size=64,\n",
    "    augment=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Extract the dataloaders and datasets\n",
    "train_loader, train_dataset = dataloaders.get('train', (None, None))\n",
    "val_loader, val_dataset = dataloaders.get('val', (None, None))\n",
    "test_loader, test_dataset = dataloaders.get('test', (None, None))\n",
    "\n",
    "# Check if datasets were loaded successfully\n",
    "if train_loader is None:\n",
    "    print(\"Warning: Training dataset not found. Please run the data preparation scripts first.\")\n",
    "else:\n",
    "    print(f\"Training dataset loaded with {len(train_dataset)} samples\")\n",
    "    \n",
    "if val_loader is None:\n",
    "    print(\"Warning: Validation dataset not found.\")\n",
    "else:\n",
    "    print(f\"Validation dataset loaded with {len(val_dataset)} samples\")\n",
    "\n",
    "# Get the number of classes from the dataset\n",
    "if train_dataset is not None:\n",
    "    num_classes = get_num_classes(train_dataset)\n",
    "    print(f\"Number of unique classes (characters): {num_classes}\")\n",
    "else:\n",
    "    # Default number of classes if dataset not available\n",
    "    num_classes = 3036\n",
    "    print(f\"Using default number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8407a1",
   "metadata": {},
   "source": [
    "## 3. CNN Model for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ea4f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KanjiCNN(nn.Module):\n",
    "    def __init__(self, num_classes=3036):\n",
    "        super(KanjiCNN, self).__init__()\n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Third convolutional block\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 1024)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        \n",
    "        # Second block\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        \n",
    "        # Third block\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        # Flatten and fully connected layers\n",
    "        x = x.view(-1, 128 * 16 * 16)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize the model with the correct number of classes\n",
    "model = KanjiCNN(num_classes=num_classes)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f425395",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bf372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=10, device=None):\n",
    "    # Set device\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Training on device: {device}\")\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Convert string labels to indices if necessary\n",
    "            if isinstance(labels[0], str):\n",
    "                # Create a mapping of characters to indices\n",
    "                unique_chars = sorted(set(labels))\n",
    "                char_to_idx = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "                # Convert labels to indices\n",
    "                label_indices = torch.tensor([char_to_idx[label] for label in labels])\n",
    "                labels = label_indices\n",
    "            \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Print batch statistics\n",
    "            if (i + 1) % 50 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_loader)}], '\n",
    "                      f'Loss: {loss.item():.4f}, Acc: {100 * correct / total:.2f}%')\n",
    "        \n",
    "        # Calculate epoch statistics\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.2f}%')\n",
    "        \n",
    "        # Validation phase\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    # Convert string labels to indices if necessary\n",
    "                    if isinstance(labels[0], str):\n",
    "                        # Create a mapping of characters to indices\n",
    "                        unique_chars = sorted(set(labels))\n",
    "                        char_to_idx = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "                        # Convert labels to indices\n",
    "                        label_indices = torch.tensor([char_to_idx[label] for label in labels])\n",
    "                        labels = label_indices\n",
    "                    \n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            val_loss = val_loss / len(val_loader)\n",
    "            val_acc = 100 * correct / total\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            \n",
    "            print(f'Validation Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%')\n",
    "            \n",
    "            # Update learning rate based on validation loss\n",
    "            scheduler.step(val_loss)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Train the model if datasets are available\n",
    "if train_loader is not None and val_loader is not None:\n",
    "    # For demonstration, we'll use a smaller number of epochs\n",
    "    num_epochs = 5\n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model, history = train_model(model, train_loader, val_loader, num_epochs=num_epochs)\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['
