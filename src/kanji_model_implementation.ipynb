{ "cells": [ { "cell_type": "markdown", "metadata": {}, "source": [ "# Simple Kanji Recognition Training\n", "\n", "This notebook loads data from prepared LMDB datasets and trains a simple kanji recognition model using existing code." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "import os\n", "import sys\n", "import torch\n", "\n", "# Check for GPU\n", "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n", "print(f"Using device: {device}")" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## Set Up Paths and Import Modules" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Set up paths\n", "IN_COLAB = 'google.colab' in sys.modules\n", "\n", "if IN_COLAB:\n", " # For Google Colab\n", " from google.colab import drive\n", " drive.mount('/content/drive')\n", " \n", " BASE_DIR = '/content'\n", " SRC_DIR = '/content/src'\n", " \n", " # Create directories\n", " !mkdir -p /content/data/ETL9G\n", " !mkdir -p /content/output/prep\n", " !mkdir -p /content/output/models\n", " \n", " # Upload source files if needed\n", " if not os.path.exists(SRC_DIR):\n", " !mkdir -p {SRC_DIR}\n", " print("Please upload your source files to the 'src' directory")\n", "else:\n", " # For local Jupyter\n", " BASE_DIR = os.path.dirname(os.getcwd())\n", " SRC_DIR = os.path.join(BASE_DIR, 'src')\n", "\n", "# Add the source directory to Python path\n", "if SRC_DIR not in sys.path:\n", " sys.path.append(SRC_DIR)\n", "\n", "OUTPUT_DIR = os.path.join(BASE_DIR, 'output', 'prep')\n", "MODEL_DIR = os.path.join(BASE_DIR, 'output', 'models')\n", "\n", "# Import project modules\n", "try:\n", " from load import get_dataloaders, get_num_classes\n", " from train import SimpleKanjiCNN, train_model\n", " print("Successfully imported modules")\n", "except ImportError as e:\n", " print(f"Error importing modules: {e}")" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## Load Data and Create Model" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Load data using existing load.py functions\n", "dataloaders = get_dataloaders(\n", " data_dir=OUTPUT_DIR,\n", " batch_size=64,\n", " augment=True,\n", " num_workers=4\n", ")\n", "\n", "train_loader, train_dataset = dataloaders['train']\n", "val_loader, val_dataset = dataloaders['val']\n", "\n", "# Get number of classes\n", "num_classes = get_num_classes(train_dataset)\n", "print(f"Datasets loaded with {num_classes} classes")\n", "print(f"Training set: {len(train_dataset)} samples")\n", "print(f"Validation set: {len(val_dataset)} samples")" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Create model using existing SimpleKanjiCNN class\n", "model = SimpleKanjiCNN(num_classes)\n", "\n", "# Print model summary\n", "total_params = sum(p.numel() for p in model.parameters())\n", "print(f"Model created with {total_params:,} total parameters")\n", "print(model)" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## Train the Model\n", "\n", "Using the existing train_model function from train.py" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Create model directory\n", "import os\n", "from datetime import datetime\n", "\n", "run_dir = os.path.join(MODEL_DIR, f"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}")\n", "os.makedirs(run_dir, exist_ok=True)\n", "\n", "# Train model using existing train_model function\n", "trained_model = train_model(\n", " model=model,\n", " train_loader=train_loader,\n", " val_loader=val_loader,\n", " num_epochs=5, # Adjust as needed\n", " device=device,\n", " output_dir=run_dir\n", ")\n", "\n", "print(f"Training completed! Model saved to {run_dir}")" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## Test the Model" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Test on test set if available\n", "if 'test' in dataloaders:\n", " test_loader, test_dataset = dataloaders['test']\n", " print(f"Test set: {len(test_dataset)} samples")\n", " \n", " # Set to evaluation mode\n", " model.eval()\n", " \n", " # Test loop\n", " correct = 0\n", " total = 0\n", " \n", " with torch.no_grad():\n", " for images, labels in test_loader:\n", " # Handle string labels if needed\n", " if isinstance(labels[0], str):\n", " continue # Skip this batch for simplicity in this basic example\n", " \n", " images, labels = images.to(device), labels.to(device)\n", " outputs = model(images)\n", " _, predicted = torch.max(outputs.data, 1)\n", " \n", " total += labels.size(0)\n", " correct += (predicted == labels).sum().item()\n", " \n", " # Print results\n", " test_acc = 100 * correct / total\n", " print(f"Test Accuracy: {test_acc:.2f}%")\n", "else:\n", " print("Test set not available")" ] } ], "metadata": { "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" }, "language_info": { "codemirror_mode": { "name": "ipython", "version": 3 }, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.16" } }, "nbformat": 4, "nbformat_minor": 4 }



